{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys,os\n",
    "from pathlib import Path\n",
    "\n",
    "from scipy.fftpack import shift\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from scipy.io.wavfile import write\n",
    "from vits.models import SynthesizerInfer\n",
    "from pitch import load_csv_pitch\n",
    "from feature_retrieval import IRetrieval, DummyRetrieval, FaissIndexRetrieval, load_retrieve_index\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "os.environ['USE_FLASH_ATTENTION']='1'\n",
    "def get_speaker_name_from_path(speaker_path: Path) -> str:\n",
    "    suffixes = \"\".join(speaker_path.suffixes)\n",
    "    filename = speaker_path.name\n",
    "    return filename.rstrip(suffixes)\n",
    "\n",
    "\n",
    "def create_retrival(cli_args) -> IRetrieval:\n",
    "    if not cli_args.enable_retrieval:\n",
    "        logger.info(\"infer without retrival\")\n",
    "        return DummyRetrieval()\n",
    "    else:\n",
    "        logger.info(\"load index retrival model\")\n",
    "\n",
    "    speaker_name = get_speaker_name_from_path(Path(args.spk))\n",
    "    base_path = Path(\".\").absolute() / \"data_svc\" / \"indexes\" / speaker_name\n",
    "    print('base_path',base_path)\n",
    "    if cli_args.hubert_index_path:\n",
    "        hubert_index_filepath = cli_args.hubert_index_path\n",
    "    else:\n",
    "        index_name = f\"{cli_args.retrieval_index_prefix}hubert.index\"\n",
    "        hubert_index_filepath = base_path / index_name\n",
    "\n",
    "    if cli_args.whisper_index_path:\n",
    "        whisper_index_filepath = cli_args.whisper_index_path\n",
    "    else:\n",
    "        index_name = f\"{cli_args.retrieval_index_prefix}whisper.index\"\n",
    "        whisper_index_filepath = base_path / index_name\n",
    "\n",
    "    return FaissIndexRetrieval(\n",
    "        hubert_index=load_retrieve_index(\n",
    "            filepath=hubert_index_filepath,\n",
    "            ratio=cli_args.retrieval_ratio,\n",
    "            n_nearest_vectors=cli_args.n_retrieval_vectors\n",
    "        ),\n",
    "        whisper_index=load_retrieve_index(\n",
    "            filepath=whisper_index_filepath,\n",
    "            ratio=cli_args.retrieval_ratio,\n",
    "            n_nearest_vectors=cli_args.n_retrieval_vectors\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def load_svc_model(checkpoint_path, model):\n",
    "    assert os.path.isfile(checkpoint_path)\n",
    "    checkpoint_dict = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    saved_state_dict = checkpoint_dict[\"model_g\"]\n",
    "    state_dict = model.state_dict()\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        try:\n",
    "            new_state_dict[k] = saved_state_dict[k]\n",
    "        except:\n",
    "            print(\"%s is not in the checkpoint\" % k)\n",
    "            new_state_dict[k] = v\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def svc_infer(model, retrieval: IRetrieval, spk, pit, ppg, vec, hp, device):\n",
    "    len_pit = pit.size()[0]\n",
    "    len_vec = vec.size()[0]\n",
    "    len_ppg = ppg.size()[0]\n",
    "    len_min = min(len_pit, len_vec)\n",
    "    len_min = min(len_min, len_ppg)\n",
    "    pit = pit[:len_min]\n",
    "    vec = vec[:len_min, :]\n",
    "    ppg = ppg[:len_min, :]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        spk = spk.unsqueeze(0).to(device)\n",
    "        source = pit.unsqueeze(0).to(device)\n",
    "        source = model.pitch2source(source)\n",
    "        pitwav = model.source2wav(source)\n",
    "        write(\"svc_out_pit.wav\", hp.data.sampling_rate, pitwav)\n",
    "        print('pit?',pitwav.shape)\n",
    "\n",
    "        hop_size = hp.data.hop_length\n",
    "        all_frame = len_min\n",
    "        hop_frame = 10\n",
    "        out_chunk = 2500  # 25 S\n",
    "        out_index = 0\n",
    "        out_audio = []\n",
    "        print(all_frame)\n",
    "        while (out_index < all_frame):\n",
    "\n",
    "            if (out_index == 0):  # start frame\n",
    "                cut_s = 0\n",
    "                cut_s_out = 0\n",
    "            else:\n",
    "                cut_s = out_index - hop_frame\n",
    "                cut_s_out = hop_frame * hop_size\n",
    "\n",
    "            if (out_index + out_chunk + hop_frame > all_frame):  # end frame\n",
    "                cut_e = all_frame\n",
    "                cut_e_out = -1\n",
    "            else:\n",
    "                cut_e = out_index + out_chunk + hop_frame\n",
    "                cut_e_out = -1 * hop_frame * hop_size\n",
    "\n",
    "            sub_ppg = retrieval.retriv_hubert(ppg[cut_s:cut_e, :])\n",
    "            sub_vec = retrieval.retriv_whisper(vec[cut_s:cut_e, :])\n",
    "            sub_ppg = sub_ppg.unsqueeze(0).to(device)\n",
    "            sub_vec = sub_vec.unsqueeze(0).to(device)\n",
    "            sub_pit = pit[cut_s:cut_e].unsqueeze(0).to(device)\n",
    "            sub_len = torch.LongTensor([cut_e - cut_s]).to(device)\n",
    "            sub_har = source[:, :, cut_s *\n",
    "                             hop_size:cut_e * hop_size].to(device)\n",
    "            sub_out = model.inference(\n",
    "                sub_ppg, sub_vec, sub_pit, spk, sub_len, sub_har)\n",
    "            print('inference?',sub_out.shape)\n",
    "            \n",
    "            sub_out = sub_out[0, 0].data.cpu().detach().numpy()\n",
    "\n",
    "            sub_out = sub_out[cut_s_out:cut_e_out]\n",
    "            out_audio.extend(sub_out)\n",
    "            out_index = out_index + out_chunk\n",
    "\n",
    "        out_audio = np.asarray(out_audio)\n",
    "        print('out?',out_audio.shape)\n",
    "    return out_audio\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    if (args.ppg == None):\n",
    "        args.ppg = \"svc_tmp.ppg.npy\"\n",
    "        print(\n",
    "            f\"Auto run : python whisper/inference.py -w {args.wave} -p {args.ppg}\")\n",
    "        os.system(f\"python whisper/inference.py -w {args.wave} -p {args.ppg}\")\n",
    "\n",
    "    if (args.vec == None):\n",
    "        args.vec = \"svc_tmp.vec.npy\"\n",
    "        print(\n",
    "            f\"Auto run : python hubert/inference.py -w {args.wave} -v {args.vec}\")\n",
    "        os.system(f\"python hubert/inference.py -w {args.wave} -v {args.vec}\")\n",
    "\n",
    "    if (args.pit == None):\n",
    "        args.pit = \"svc_tmp.pit.csv\"\n",
    "        print(\n",
    "            f\"Auto run : python pitch/inference.py -w {args.wave} -p {args.pit}\")\n",
    "        os.system(f\"python pitch/inference.py -w {args.wave} -p {args.pit}\")\n",
    "\n",
    "    if args.debug:\n",
    "        logging.basicConfig(level=logging.DEBUG)\n",
    "    else:\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    hp = OmegaConf.load(args.config)\n",
    "    model = SynthesizerInfer(\n",
    "        hp.data.filter_length // 2 + 1,\n",
    "        hp.data.segment_size // hp.data.hop_length,\n",
    "        hp)\n",
    "    load_svc_model(args.model, model)\n",
    "    retrieval = create_retrival(args)\n",
    "    print(retrieval)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    spk = np.load(args.spk)\n",
    "    spk = torch.FloatTensor(spk)\n",
    "\n",
    "    ppg = np.load(args.ppg)\n",
    "    ppg = np.repeat(ppg, 2, 0)  # 320 PPG -> 160 * 2\n",
    "    ppg = torch.FloatTensor(ppg)\n",
    "    # ppg = torch.zeros_like(ppg)\n",
    "\n",
    "    vec = np.load(args.vec)\n",
    "    vec = np.repeat(vec, 2, 0)  # 320 PPG -> 160 * 2\n",
    "    vec = torch.FloatTensor(vec)\n",
    "    # vec = torch.zeros_like(vec)\n",
    "\n",
    "    pit = load_csv_pitch(args.pit)\n",
    "    print(\"pitch shift: \", args.shift)\n",
    "    if (args.shift == 0):\n",
    "        pass\n",
    "    else:\n",
    "        pit = np.array(pit)\n",
    "        source = pit[pit > 0]\n",
    "        source_ave = source.mean()\n",
    "        source_min = source.min()\n",
    "        source_max = source.max()\n",
    "        print(f\"source pitch statics: mean={source_ave:0.1f}, \\\n",
    "                min={source_min:0.1f}, max={source_max:0.1f}\")\n",
    "        shift = args.shift\n",
    "        shift = 2 ** (shift / 12)\n",
    "        pit = pit * shift\n",
    "    pit = torch.FloatTensor(pit)\n",
    "\n",
    "    out_audio = svc_infer(model, retrieval, spk, pit, ppg, vec, hp, device)\n",
    "    write(f\"{args.out}.wav\", hp.data.sampling_rate, out_audio)\n",
    "\n",
    "\n",
    "class args:\n",
    "    config = 'configs/base.yaml'\n",
    "    model = 'soyo_avg.pth'\n",
    "    spk = r'./data_svc\\speaker\\soyo\\soyo夹_51.spk.npy'                 \n",
    "    #'./data_svc\\speaker\\soyo\\キリトリセン_6.spk.npy' \n",
    "    wave = \"1528567343取.wav\"\n",
    "    shift = 0\n",
    "    out = '1528567343'\n",
    "    ppg = None\n",
    "    vec = None\n",
    "    pit = None\n",
    "    enable_retrieval = False\n",
    "    retrieval_index_prefix = ''\n",
    "    hubert_index_path = None\n",
    "    whisper_index_path = None\n",
    "    \n",
    "    retrieval_ratio = 0.5\n",
    "    n_retrieval_vectors = 3\n",
    "    \n",
    "    debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto run : python whisper/inference.py -w 1528567343取.wav -p svc_tmp.ppg.npy\n",
      "Auto run : python hubert/inference.py -w 1528567343取.wav -v svc_tmp.vec.npy\n",
      "Auto run : python pitch/inference.py -w 1528567343取.wav -p svc_tmp.pit.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:infer without retrival\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<feature_retrieval.retrieval.DummyRetrieval object at 0x000002608B2CD1F0>\n",
      "pitch shift:  0\n",
      "pit? (120320,)\n",
      "376\n",
      "inference? torch.Size([1, 1, 120320])\n",
      "out? (120319,)\n"
     ]
    }
   ],
   "source": [
    "main(args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc in wave : 詩超絆1.wav\n",
      "svc out wave : svc_out.wav\n",
      "svc post wave : svc_out_post.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Data\\Code\\so-vits-svc-5.0\\svc_inference_post.py:12: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  x, sr = librosa.load(file, sr=sr)\n",
      "e:\\.env\\sovit5\\lib\\site-packages\\librosa\\core\\audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\.env\\sovit5\\lib\\site-packages\\librosa\\core\\audio.py\", line 175, in load\n",
      "    y, sr_native = __soundfile_load(path, offset, duration, dtype)\n",
      "  File \"e:\\.env\\sovit5\\lib\\site-packages\\librosa\\core\\audio.py\", line 208, in __soundfile_load\n",
      "    context = sf.SoundFile(path)\n",
      "  File \"e:\\.env\\sovit5\\lib\\site-packages\\soundfile.py\", line 658, in __init__\n",
      "    self._file = self._open(file, mode_int, closefd)\n",
      "  File \"e:\\.env\\sovit5\\lib\\site-packages\\soundfile.py\", line 1216, in _open\n",
      "    raise LibsndfileError(err, prefix=\"Error opening {0!r}: \".format(self.name))\n",
      "soundfile.LibsndfileError: Error opening '詩超絆1.wav': System error.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\Data\\Code\\so-vits-svc-5.0\\svc_inference_post.py\", line 34, in <module>\n",
      "    ref_wave = load_audio(args.ref, sr=16000)\n",
      "  File \"e:\\Data\\Code\\so-vits-svc-5.0\\svc_inference_post.py\", line 12, in load_audio\n",
      "    x, sr = librosa.load(file, sr=sr)\n",
      "  File \"e:\\.env\\sovit5\\lib\\site-packages\\librosa\\core\\audio.py\", line 183, in load\n",
      "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "  File \"e:\\.env\\sovit5\\lib\\site-packages\\decorator.py\", line 232, in fun\n",
      "    return caller(func, *(extras + args), **kw)\n",
      "  File \"e:\\.env\\sovit5\\lib\\site-packages\\librosa\\util\\decorators.py\", line 59, in __wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"e:\\.env\\sovit5\\lib\\site-packages\\librosa\\core\\audio.py\", line 239, in __audioread_load\n",
      "    reader = audioread.audio_open(path)\n",
      "  File \"e:\\.env\\sovit5\\lib\\site-packages\\audioread\\__init__.py\", line 127, in audio_open\n",
      "    return BackendClass(path)\n",
      "  File \"e:\\.env\\sovit5\\lib\\site-packages\\audioread\\rawread.py\", line 59, in __init__\n",
      "    self._fh = open(filename, 'rb')\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '詩超絆1.wav'\n"
     ]
    }
   ],
   "source": [
    "!python svc_inference_post.py --ref 詩超絆1.wav --svc svc_out.wav --out svc_out_post.wav"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sovit5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
